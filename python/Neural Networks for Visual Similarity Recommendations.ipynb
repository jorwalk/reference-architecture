{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keras' Pretrained Neural Networks for Visual Similarity Recommendations\n",
    "\n",
    "Explore an unsupervised, deep learning-based model. You'll find that the implementation is fairly simple with remarkably promising results which is almost a smack in the face to all of that effort put in earlier.\n",
    "We are going to build a model-to-model recommender using thumbnail images for models as our input and the visual similarity between models as our recommendation score. I was inspired to do this after reading Christopher Bonnett's post on product classification, so we will follow a similar approach.\n",
    "\n",
    "Since our goal is to measure visual similarity, we will need to generate features from our images and then calculate some similarity measure between different images using said features. Back in the day, maybe one would employ fancy wavelets or SIFT keypoints or something for creating features, but this is the Era of Deep Learning and manual feature extraction is for old people.\n",
    "\n",
    "Staying on-trend, we will use a pretrained neural network (NN) to extract features. The NN was originally trained to classify images among 1000 labels (e.g. \"dog\", \"train\", etc...). We'll chop off the last 3 fully-connected layers of the network which do the final mapping between deep features and class labels and use the fourth-to-last layer as a long feature vector describing our images.\n",
    "\n",
    "Thankfully, all of this is extremely simple to do with the pretrained models in Keras. Keras allows one to easily build deep learning models on top of either Tensorflow or Theano. Keras also now comes with pretrained models that can be loaded and used. For more information about the available models, visit the Applications section of the documentation. For our purposes, we'll use the VGG16 model because that's what other people seemed to use and I don't know enough to have a compelling reason to stray from the norm.\n",
    "\n",
    "Our task is now as follows:\n",
    "\n",
    "1. Load and process images\n",
    "2. Feed images through NN.\n",
    "3. Calculate image similarities.\n",
    "4. Recommend models!\n",
    "## Load and process images\n",
    "The first step, which we won't go through here, was to download all of the image thumbnails. There seems to be a standard thumbnail for each Sketchfab model accessible via their API, so I added a function to the rec-a-sketch crawl.py script to automate downloading of all the thumbnails.\n",
    "Let's load in our libraries and take a look at one of these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from IPython.display import display, Image, HTML\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image as kimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix as little_matrix\n",
    "import skimage.io\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './img/0fb30f82-d82a-49a2-850a-6bebcb9137f2.jpg'\n",
    "img = skimage.io.imread(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = kimage.load_img(file, target_size=(224, 224))\n",
    "x = kimage.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the image can be represented as a 3D matrix through two spatial dimensions (200 x 200) and then a third RGB dimension. We have to do a couple of preprocessing steps before feeding an image through the VGG16 model. The images must be resized to 224 x 224, the color channels must be normalized, and an extra dimension must be added due to Keras expecting to recieve multiple models. Thankfully, Keras has built-in functions to handle most of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load our model in and try feeding the image through.\n",
    "(VGG16 model for Keras)[https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3]\n",
    "This is the Keras model of the 16-layer network used by the VGG team in the ILSVRC-2014 competition.\n",
    "\n",
    "It has been obtained by directly converting the Caffe model provived by the authors.\n",
    "\n",
    "Details about the network architecture can be found in the following arXiv paper:\n",
    "\n",
    "Very Deep Convolutional Networks for Large-Scale Image Recognition\n",
    "K. Simonyan, A. Zisserman\n",
    "arXiv:1409.1556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_top=False removes final connected layers\n",
    "model = VGG16(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x)\n",
    "print(pred.shape)\n",
    "print(pred.ravel().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will later have to flatten the output of the model into a long feature vector. One thing that should be noted is the time that it takes to run a single model though the NN on my 4-core machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n25\n",
    "pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed images through NN\n",
    "With our set of valid model IDs in hand, we can now run through the long process of loading in all of the image files, preprocessing them, and running them through the VGG prediction. This takes a long time, and certain steps blowup memory. I've decided to batch things up below and include some print statements so that one can track progress. Beware: this takes a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory='img'\n",
    "items=[]\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        items.append(filename)\n",
    "\n",
    "idx_to_mid = {}\n",
    "batch_size = 25\n",
    "min_idx = 0\n",
    "max_idx = min_idx + batch_size\n",
    "total_max = len(items)\n",
    "print(total_max)\n",
    "n_dims = pred.ravel().shape[0]\n",
    "px = 224\n",
    "print(n_dims)\n",
    "\n",
    "# Initialize predictions matrix\n",
    "preds = little_matrix((len(items), n_dims))\n",
    "\n",
    "import PIL\n",
    "\n",
    "while min_idx < total_max - 1:\n",
    "    t0 = time.time()\n",
    "    X = np.zeros(((max_idx - min_idx), px, px, 3))\n",
    "    # For each file in batch, \n",
    "    # load as row into X\n",
    "    for i in range(min_idx, max_idx):\n",
    "        print(i)\n",
    "        item = items[i]\n",
    "        idx_to_mid[i] = item\n",
    "        \n",
    "        image='./'+ directory +'/'+item\n",
    "        img = kimage.load_img(image, target_size=(px, px))\n",
    "        img_array = kimage.img_to_array(img)\n",
    "               \n",
    "        X[i - min_idx, :, :, :] = img_array\n",
    "        if i % 200 == 0 and i != 0:\n",
    "            t1 = time.time()\n",
    "            print('{}: {}'.format(i, (t1 - t0) / i))\n",
    "            t0 = time.time()\n",
    "            \n",
    "    max_idx = i\n",
    "    t1 = time.time()\n",
    "    print('{}: {}'.format(i, (t1 - t0) / i))\n",
    "    \n",
    "    print('Preprocess input')\n",
    "    t0 = time.time()\n",
    "    X = preprocess_input(X)\n",
    "    t1 = time.time()\n",
    "    print('{}'.format(t1 - t0))\n",
    "    \n",
    "    print('Predicting')\n",
    "    t0 = time.time()\n",
    "    these_preds = model.predict(X)\n",
    "    shp = ((max_idx - min_idx) + 1, n_dims)\n",
    "    \n",
    "    # Place predictions inside full preds matrix.\n",
    "    preds[min_idx:max_idx + 1, :] = these_preds.reshape(shp)\n",
    "    t1 = time.time()\n",
    "    print('{}'.format(t1 - t0))\n",
    "    \n",
    "    min_idx = max_idx\n",
    "    max_idx = np.min((max_idx + batch_size, total_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(ratings):\n",
    "    sim = ratings.dot(ratings.T)\n",
    "    if not isinstance(sim, np.ndarray):\n",
    "        sim = sim.toarray()\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "    return (sim / norms / norms.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a copy of this matrix in Compressed Sparse Row format\n",
    "# from keras.models import load_model\n",
    "# model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# del model  # deletes the existing model\n",
    "# returns a compiled model\n",
    "\n",
    "# model = load_model('my_model.h5')\n",
    "\n",
    "preds = preds.tocsr()\n",
    "# save the model\n",
    "\n",
    "# Compute cosine similarity\n",
    "sim = cosine_similarity(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thumbnails(sim, idx, idx_to_mid, N=10):\n",
    "    row = sim[idx, :]\n",
    "    thumbs = []\n",
    "    mids = []\n",
    "    for x in np.argsort(-row)[:N]:\n",
    "        thumbs.append(idx_to_mid[x])\n",
    "        mids.append(idx_to_mid[x])\n",
    "    return thumbs, mids\n",
    "\n",
    "def display_thumbs(thumbs, mids, N=5):\n",
    "    thumb_html = \"<a href='{}' target='_blank'>\\\n",
    "                  <img style='width: 300px; margin: 0px; \\\n",
    "                  float: left; border: 1px solid black; display:inline-block' \\\n",
    "                  src='./img/{}' /></a>\"\n",
    "    images = \"<div class='line' style='max-width: 1024px; display: block;'>\"\n",
    "    display(HTML('<font size=5>'+'Input Model'+'</font>'))\n",
    "    link = './'+ directory +'/{}'.format(mids[0])\n",
    "    url = thumbs[0]\n",
    "    display(HTML(thumb_html.format(link, url)))\n",
    "    display(HTML('<font size=5>'+'Similar Models'+'</font>'))\n",
    "\n",
    "    for (url, mid) in zip(thumbs[1:N+1], mids[1:N+1]):\n",
    "        link = './'+ directory +'/{}'.format(mid)\n",
    "        images += thumb_html.format(link, url)\n",
    "\n",
    "    images += '</div>'\n",
    "    display(HTML(images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "# pprint.pprint(idx_to_mid)\n",
    "# print(sim)\n",
    "display_thumbs(*get_thumbnails(sim, 290, idx_to_mid, N=14), N=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
