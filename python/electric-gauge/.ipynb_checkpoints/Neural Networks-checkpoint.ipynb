{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageNet: VGGNet, ResNet, Inception, and Xception with Keras\n",
    "\n",
    "Explore an unsupervised, deep learning-based model. \n",
    "\n",
    "We are going to build a model-to-model recommender using thumbnail images for models as our input and the visual similarity between models as our recommendation score. \n",
    "\n",
    "Since our goal is to measure visual similarity, we will need to generate features from our images and then calculate some similarity measure between different images using said features. Back in the day, maybe one would employ fancy wavelets or SIFT keypoints or something for creating features, but this is the Era of Deep Learning and manual feature extraction is for old people.\n",
    "\n",
    "Staying on-trend, we will use a pretrained neural network (NN) to extract features. The NN was originally trained to classify images among 1000 labels (e.g. \"dog\", \"train\", etc...). We'll chop off the last 3 fully-connected layers of the network which do the final mapping between deep features and class labels and use the fourth-to-last layer as a long feature vector describing our images.\n",
    "\n",
    "Thankfully, all of this is extremely simple to do with the pretrained models in Keras. Keras allows one to easily build deep learning models on top of either Tensorflow or Theano. Keras also now comes with pretrained models that can be loaded and used. For more information about the available models, visit the Applications section of the documentation. For our purposes, we'll use the VGG16 model because that's what other people seemed to use and I don't know enough to have a compelling reason to stray from the norm.\n",
    "\n",
    "The goal of this image classification challenge is to train a model that can correctly classify an input image into 1,000 separate object categories.\n",
    "\n",
    "Models are trained on ~1.2 million training images with another 50,000 images for validation and 100,000 images for testing.\n",
    "\n",
    "These 1,000 image categories represent object classes that we encounter in our day-to-day lives, such as species of dogs, cats, various household objects, vehicle types, and much more. You can find the full list of object categories in the ILSVRC challenge here.\n",
    "\n",
    "When it comes to image classification, the ImageNet challenge is the de facto benchmark for computer vision classification algorithms — and the leaderboard for this challenge has been dominated by Convolutional Neural Networks and deep learning techniques since 2012.\n",
    "\n",
    "The state-of-the-art pre-trained networks included in the Keras core library represent some of the highest performing Convolutional Neural Networks on the ImageNet challenge over the past few years. These networks also demonstrate a strong ability to generalize to images outside the ImageNet dataset via transfer learning, such as feature extraction and fine-tuning.\n",
    "\n",
    "### What is ImageNet?\n",
    "ImageNet is formally a project aimed at (manually) labeling and categorizing images into almost 22,000 separate object categories for the purpose of computer vision research.\n",
    "\n",
    "However, when we hear the term “ImageNet” in the context of deep learning and Convolutional Neural Networks, we are likely referring to the ImageNet Large Scale Visual Recognition Challenge, or ILSVRC for short.\n",
    "\n",
    "The goal of this image classification challenge is to train a model that can correctly classify an input image into 1,000 separate object categories.\n",
    "\n",
    "Models are trained on ~1.2 million training images with another 50,000 images for validation and 100,000 images for testing.\n",
    "\n",
    "These 1,000 image categories represent object classes that we encounter in our day-to-day lives, such as species of dogs, cats, various household objects, vehicle types, and much more. You can find the full list of object categories in the ILSVRC challenge here.\n",
    "\n",
    "When it comes to image classification, the ImageNet challenge is the de facto benchmark for computer vision classification algorithms — and the leaderboard for this challenge has been dominated by Convolutional Neural Networks and deep learning techniques since 2012.\n",
    "\n",
    "The state-of-the-art pre-trained networks included in the Keras core library represent some of the highest performing Convolutional Neural Networks on the ImageNet challenge over the past few years. These networks also demonstrate a strong ability to generalize to images outside the ImageNet dataset via transfer learning, such as feature extraction and fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 & VGG19\n",
    "In 2014, 16 and 19 layer networks were considered very deep (although we now have the ResNet architecture which can be successfully trained at depths of 50-200 for ImageNet and over 1,000 for CIFAR-10).\n",
    "\n",
    "Simonyan and Zisserman found training VGG16 and VGG19 challenging (specifically regarding convergence on the deeper networks), so in order to make training easier, they first trained smaller versions of VGG with less weight layers (columns A and C) first.\n",
    "\n",
    "The smaller networks converged and were then used as initializations for the larger, deeper networks — this process is called pre-training.\n",
    "\n",
    "While making logical sense, pre-training is a very time consuming, tedious task, requiring an entire network to be trained before it can serve as an initialization for a deeper network.\n",
    "\n",
    "We no longer use pre-training (in most cases) and instead prefer Xaiver/Glorot initialization or MSRA initialization (sometimes called He et al. initialization from the paper, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification). You can read more about the importance of weight initialization and the convergence of deep neural networks inside All you need is a good init, Mishkin and Matas (2015).\n",
    "\n",
    "Unfortunately, there are two major drawbacks with VGGNet:\n",
    "\n",
    "It is painfully slow to train.\n",
    "The network architecture weights themselves are quite large (in terms of disk/bandwidth).\n",
    "Due to its depth and number of fully-connected nodes, VGG is over 533MB for VGG16 and 574MB for VGG19. This makes deploying VGG a tiresome task.\n",
    "\n",
    "We still use VGG in many deep learning image classification problems; however, smaller network architectures are often more desirable (such as SqueezeNet, GoogLeNet, etc.).\n",
    "\n",
    "\n",
    "## ResNet50\n",
    "Unlike traditional sequential network architectures such as AlexNet, OverFeat, and VGG, ResNet is instead a form of “exotic architecture” that relies on micro-architecture modules (also called “network-in-network architectures”).\n",
    "\n",
    "The term micro-architecture refers to the set of “building blocks” used to construct the network. A collection of micro-architecture building blocks (along with your standard CONV, POOL, etc. layers) leads to the macro-architecture (i.e,. the end network itself).\n",
    "\n",
    "First introduced by He et al. in their 2015 paper, Deep Residual Learning for Image Recognition, the ResNet architecture has become a seminal work, demonstrating that extremely deep networks can be trained using standard SGD (and a reasonable initialization function) through the use of residual modules:\n",
    "\n",
    "Further accuracy can be obtained by updating the residual module to use identity mappings, as demonstrated in their 2016 followup publication, Identity Mappings in Deep Residual Networks:\n",
    "\n",
    "That said, keep in mind that the ResNet50 (as in 50 weight layers) implementation in the Keras core is based on the former 2015 paper.\n",
    "\n",
    "Even though ResNet is much deeper than VGG16 and VGG19, the model size is actually substantially smaller due to the usage of global average pooling rather than fully-connected layers — this reduces the model size down to 102MB for ResNet50.\n",
    "\n",
    "## Inception V3\n",
    "The “Inception” micro-architecture was first introduced by Szegedy et al. in their 2014 paper, Going Deeper with Convolutions:\n",
    "\n",
    "The goal of the inception module is to act as a “multi-level feature extractor” by computing 1×1, 3×3, and 5×5 convolutions within the same module of the network — the output of these filters are then stacked along the channel dimension and before being fed into the next layer in the network.\n",
    "\n",
    "The original incarnation of this architecture was called GoogLeNet, but subsequent manifestations have simply been called Inception vN where N refers to the version number put out by Google.\n",
    "\n",
    "The Inception V3 architecture included in the Keras core comes from the later publication by Szegedy et al., Rethinking the Inception Architecture for Computer Vision (2015) which proposes updates to the inception module to further boost ImageNet classification accuracy.\n",
    "\n",
    "The weights for Inception V3 are smaller than both VGG and ResNet, coming in at 96MB.\n",
    "\n",
    "## Xception\n",
    "Xception was proposed by none other than François Chollet himself, the creator and chief maintainer of the Keras library.\n",
    "\n",
    "Xception is an extension of the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions.\n",
    "\n",
    "The original publication, Xception: Deep Learning with Depthwise Separable Convolutions can be found here.\n",
    "\n",
    "Xception sports the smallest weight serialization at only 91MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, requests, os, glob, pickle, time, PIL, pprint\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, Image, HTML\n",
    "from keras.models import load_model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.applications import Xception # TensorFlow ONLY\n",
    "from keras.applications import VGG19\n",
    "from keras.applications import InceptionResNetV2\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.preprocessing import image as kimage\n",
    "from scipy.sparse import lil_matrix as little_matrix\n",
    "from skimage import io\n",
    "from matplotlib import pyplot\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = './gauges/gauge_rot_3.png'\n",
    "img = io.imread(file)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename=file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "img = kimage.load_img(file, target_size=(224, 224))\n",
    "x = kimage.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the image can be represented as a 3D matrix through two spatial dimensions (224 x 224) and then a third RGB dimension. We have to do a couple of preprocessing steps before feeding an image through the model. The images must be resized to 224 x 224, the color channels must be normalized, and an extra dimension must be added due to Keras expecting to recieve multiple models. Thankfully, Keras has built-in functions to handle most of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define NN models with imagenet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 28s 0us/step\n",
      "219070464/219055592 [==============================] - 28s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model_vgg16 = VGG16(include_top=False, weights='imagenet')\n",
    "model_vgg19 = VGG19(include_top=False, weights='imagenet')\n",
    "model_resnet50 = ResNet50(include_top=False, weights='imagenet')\n",
    "model_inception = InceptionV3(include_top=False, weights='imagenet')\n",
    "model_inception_res_net = InceptionResNetV2(include_top=False, weights='imagenet')\n",
    "model_xception = Xception(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction for one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 512)\n",
      "(25088,)\n"
     ]
    }
   ],
   "source": [
    "# VGG 16\n",
    "pred_vgg16 = model_vgg16.predict(x)\n",
    "print(pred_vgg16.shape)\n",
    "print(pred_vgg16.ravel().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 512)\n",
      "(25088,)\n"
     ]
    }
   ],
   "source": [
    "# VGG 19\n",
    "pred_vgg19 = model_vgg19.predict(x)\n",
    "print(pred_vgg19.shape)\n",
    "# Return a contiguous flattened array.\n",
    "print(pred_vgg19.ravel().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 2048)\n",
      "(100352,)\n"
     ]
    }
   ],
   "source": [
    "# ResNet50\n",
    "pred_resnet50 = model_resnet50.predict(x)\n",
    "print(pred_resnet50.shape)\n",
    "# Return a contiguous flattened array.\n",
    "print(pred_resnet50.ravel().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 5, 2048)\n",
      "(51200,)\n"
     ]
    }
   ],
   "source": [
    "# InceptionV3\n",
    "pred_inception = model_inception.predict(x)\n",
    "print(pred_inception.shape)\n",
    "print(pred_inception.ravel().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 2048)\n",
      "(100352,)\n"
     ]
    }
   ],
   "source": [
    "# Xception\n",
    "pred_xception = model_xception.predict(x)\n",
    "print(pred_xception.shape)\n",
    "print(pred_xception.ravel().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 5, 1536)\n",
      "(38400,)\n"
     ]
    }
   ],
   "source": [
    "# InceptionResNetV2\n",
    "pred_inception_res_net = model_inception_res_net.predict(x)\n",
    "print(pred_inception_res_net.shape)\n",
    "print(pred_inception_res_net.ravel().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long does it take to run a single model through the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 loops, best of 3: 214 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n25\n",
    "pred_vgg16 = model_vgg16.predict(x)\n",
    "# VGG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 loops, best of 3: 275 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n25\n",
    "pred_vgg19 = model_vgg19.predict(x) \n",
    "# VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 loops, best of 3: 197 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n25\n",
    "pred_resnet50 = model_resnet50.predict(x) \n",
    "# ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 loops, best of 3: 114 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n25\n",
    "pred_inception = model_inception.predict(x)\n",
    "# InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 loops, best of 3: 167 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n25\n",
    "pred_xception = model_xception.predict(x)\n",
    "# Xception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed images through NN\n",
    "With our set of valid model IDs in hand, we can now run through the long process of loading in all of the image files, preprocessing them, and running them through the VGG prediction. This takes a long time, and certain steps blowup memory. I've decided to batch things up below and include some print statements so that one can track progress. Beware: this takes a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_mid = {}\n",
    "directory = 'gauges'\n",
    "\n",
    "def train_neural_network(pred, model):\n",
    "    # Iterate over the folder to catalog the images\n",
    "    items=[]\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            items.append(filename)\n",
    "    # Define variables for training\n",
    "    batch_size = 360\n",
    "    min_idx = 0\n",
    "    max_idx = min_idx + batch_size\n",
    "    total_max = len(items)\n",
    "    print(total_max)\n",
    "    n_dims = pred.ravel().shape[0]\n",
    "    px = 224\n",
    "    # Initialize predictions matrix\n",
    "    preds = little_matrix((len(items), n_dims))\n",
    "\n",
    "    while min_idx < total_max - 1:\n",
    "        t0 = time.time()\n",
    "        X = np.zeros(((max_idx - min_idx), px, px, 3))\n",
    "        # For each file in batch, \n",
    "        # load as row into X\n",
    "        for i in range(min_idx, max_idx):\n",
    "            item = items[i]\n",
    "            idx_to_mid[i] = item\n",
    "\n",
    "            image='./'+ directory +'/'+item\n",
    "            img = kimage.load_img(image, target_size=(px, px))\n",
    "            img_array = kimage.img_to_array(img)\n",
    "\n",
    "            X[i - min_idx, :, :, :] = img_array\n",
    "            if i % 200 == 0 and i != 0:\n",
    "                t1 = time.time()\n",
    "                print('{}: {}'.format(i, (t1 - t0) / i))\n",
    "                t0 = time.time()\n",
    "\n",
    "        max_idx = i\n",
    "        t1 = time.time()\n",
    "        print('{}: {}'.format(i, (t1 - t0) / i))\n",
    "\n",
    "        t0 = time.time()\n",
    "        X = preprocess_input(X)\n",
    "        t1 = time.time()\n",
    "        print('Preprocess input time: {}'.format(t1 - t0))\n",
    "\n",
    "        print('Predicting')\n",
    "        t0 = time.time()\n",
    "        these_preds = model.predict(X)\n",
    "        shp = ((max_idx - min_idx) + 1, n_dims)\n",
    "\n",
    "        # Place predictions inside full preds matrix.\n",
    "        preds[min_idx:max_idx + 1, :] = these_preds.reshape(shp)\n",
    "\n",
    "        t1 = time.time()\n",
    "        print('Total time: {}'.format(t1 - t0))\n",
    "\n",
    "        min_idx = max_idx\n",
    "        max_idx = np.min((max_idx + batch_size, total_max))\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The point of using Euclidean similarity metrics in vector space co-embeddings is that it reduces the \n",
    "# recommendation problem to one of finding the nearest neighbors, which can be done efficiently both \n",
    "# exactly and approximately. \n",
    "\n",
    "def cosine_similarity(ratings):\n",
    "    sim = ratings.dot(ratings.T)\n",
    "    if not isinstance(sim, np.ndarray):\n",
    "        sim = sim.toarray()\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "    return (sim / norms / norms.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "def model_visualization(model, readable):\n",
    "    filename = '{0}_model.png'.format(readable)\n",
    "    plot_model(model, to_file=filename)\n",
    "\n",
    "def model_visualization_inline(model):\n",
    "    return SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thumbnails(sim, idx, idx_to_mid, N=10):\n",
    "    row = sim[idx, :]\n",
    "    thumbs = []\n",
    "    mids = []\n",
    "    for x in np.argsort(-row)[:N]:\n",
    "        thumbs.append(idx_to_mid[x])\n",
    "        mids.append(idx_to_mid[x])\n",
    "    return thumbs, mids\n",
    "\n",
    "def display_thumbs(thumbs, mids, N=5):\n",
    "    directory = 'gauges'\n",
    "    thumb_html = \"<a href='{0}' target='_blank'><p>{2}</p><img style='width: 300px; margin: 0px; \\\n",
    "                  float: left; border: 1px solid black; display:inline-block' \\\n",
    "                  src='./gauges/{1}' /></a>\"\n",
    "    images = \"<div class='line' style='max-width: 1024px; display: block;'>\"\n",
    "    display(HTML('<font size=5>'+'Input Model'+'</font>'))\n",
    "    link = './'+ directory +'/{}'.format(mids[0])\n",
    "    url = thumbs[0]\n",
    "    display(HTML(thumb_html.format(link, url, url)))\n",
    "    display(HTML('<font size=5>'+'Similar Models'+'</font>'))\n",
    "\n",
    "    for (url, mid) in zip(thumbs[1:N+1], mids[1:N+1]):\n",
    "        link = './'+ directory +'/{}'.format(mid)\n",
    "        images += thumb_html.format(link, url, url)\n",
    "\n",
    "    images += '</div>'\n",
    "    display(HTML(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(pred, model, readable):\n",
    "    preds_train = train_neural_network(pred, model)\n",
    "    # Return a copy of this matrix in Compressed Sparse Row format\n",
    "    preds = preds_train.tocsr()\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sim = cosine_similarity(preds)\n",
    "    \n",
    "    # Create a .png visualization\n",
    "    model_visualization(model, readable)\n",
    "    \n",
    "    model_visualization_inline(model)\n",
    "    \n",
    "    display_thumbs(*get_thumbnails(sim, 210, idx_to_mid, N=6), N=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG 16\n",
    "neural_network(pred_vgg16, model_vgg16, 'vgg16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG 19\n",
    "neural_network(pred_vgg19, model_vgg19, 'vgg19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50\n",
    "neural_network(pred_resnet50, model_resnet50, 'resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361\n",
      "200: 0.00198500037193\n",
      "359: 0.000838797072514\n",
      "Preprocess input time: 0.122531890869\n",
      "Predicting\n",
      "Total time: 42.0574810505\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font size=5>Input Model</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='./gauges/gauge_rot_120.png' target='_blank'><p>gauge_rot_120.png</p><img style='width: 300px; margin: 0px;                   float: left; border: 1px solid black; display:inline-block'                   src='./gauges/gauge_rot_120.png' /></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<font size=5>Similar Models</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class='line' style='max-width: 1024px; display: block;'><a href='./gauges/gauge_rot_121.png' target='_blank'><p>gauge_rot_121.png</p><img style='width: 300px; margin: 0px;                   float: left; border: 1px solid black; display:inline-block'                   src='./gauges/gauge_rot_121.png' /></a><a href='./gauges/gauge_rot_119.png' target='_blank'><p>gauge_rot_119.png</p><img style='width: 300px; margin: 0px;                   float: left; border: 1px solid black; display:inline-block'                   src='./gauges/gauge_rot_119.png' /></a><a href='./gauges/gauge_rot_118.png' target='_blank'><p>gauge_rot_118.png</p><img style='width: 300px; margin: 0px;                   float: left; border: 1px solid black; display:inline-block'                   src='./gauges/gauge_rot_118.png' /></a><a href='./gauges/gauge_rot_122.png' target='_blank'><p>gauge_rot_122.png</p><img style='width: 300px; margin: 0px;                   float: left; border: 1px solid black; display:inline-block'                   src='./gauges/gauge_rot_122.png' /></a><a href='./gauges/gauge_rot_124.png' target='_blank'><p>gauge_rot_124.png</p><img style='width: 300px; margin: 0px;                   float: left; border: 1px solid black; display:inline-block'                   src='./gauges/gauge_rot_124.png' /></a></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# InceptionV3\n",
    "neural_network(pred_inception, model_inception, 'inception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xception\n",
    "neural_network(pred_xception, model_xception, 'xception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361\n",
      "200: 0.00200429439545\n",
      "359: 0.000614526212049\n",
      "Preprocess input time: 0.107841014862\n",
      "Predicting\n",
      "Total time: 73.0509848595\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font size=5>Input Model</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='./gauges/gauge_rot_120.png' target='_blank'><p>gauge_rot_120.png</p><img style='width: 300px; margin: 0px;                   float: left; border: 1px solid black; display:inline-block'                   src='./gauges/gauge_rot_120.png' /></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<font size=5>Similar Models</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class='line' style='max-width: 1024px; display: block;'><a href='./gauges/gauge_rot_116.png' target='_blank'><p>gauge_rot_116.png</p><img style='width: 300px; margin: 0px;                   float: left; border: 1px solid black; display:inline-block'                   src='./gauges/gauge_rot_116.png' /></a><a href='./gauges/gauge_rot_117.png' target='_blank'><p>gauge_rot_117.png</p><img style='width: 300px; margin: 0px;                   float: left; border: 1px solid black; display:inline-block'                   src='./gauges/gauge_rot_117.png' /></a><a href='./gauges/gauge_rot_124.png' target='_blank'><p>gauge_rot_124.png</p><img style='width: 300px; margin: 0px;                   float: left; border: 1px solid black; display:inline-block'                   src='./gauges/gauge_rot_124.png' /></a><a href='./gauges/gauge_rot_113.png' target='_blank'><p>gauge_rot_113.png</p><img style='width: 300px; margin: 0px;                   float: left; border: 1px solid black; display:inline-block'                   src='./gauges/gauge_rot_113.png' /></a><a href='./gauges/gauge_rot_115.png' target='_blank'><p>gauge_rot_115.png</p><img style='width: 300px; margin: 0px;                   float: left; border: 1px solid black; display:inline-block'                   src='./gauges/gauge_rot_115.png' /></a></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# InceptionResNetV2\n",
    "neural_network(pred_inception_res_net, model_inception_res_net, 'inception_res_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[   0.        330.46585    48.212666 ...    0.          0.\n",
      "      30.828505]\n",
      "   [   0.        148.0372      0.       ...    0.          0.\n",
      "     163.17685 ]\n",
      "   [   0.        377.6498      0.       ...  159.14447     0.\n",
      "       0.      ]\n",
      "   ...\n",
      "   [   0.          0.          0.       ...   30.9662      0.\n",
      "       0.      ]\n",
      "   [   0.         11.793795    0.       ... 1179.2424      0.\n",
      "     458.02097 ]\n",
      "   [ 586.90753   139.57101    56.207417 ... 1356.8796    997.93134\n",
      "     147.61572 ]]\n",
      "\n",
      "  [[   0.        126.60369     0.       ...    0.          0.\n",
      "     419.238   ]\n",
      "   [   0.          0.         32.50575  ...    0.          0.\n",
      "     633.5808  ]\n",
      "   [   0.          0.          0.       ...  293.64224     0.\n",
      "     606.61035 ]\n",
      "   ...\n",
      "   [   0.          0.         10.168309 ...  441.35013     0.\n",
      "     259.38687 ]\n",
      "   [   0.          0.        210.80286  ... 1550.4058      0.\n",
      "     690.337   ]\n",
      "   [   5.44657     0.        608.5773   ...  236.36578     0.\n",
      "     329.18216 ]]\n",
      "\n",
      "  [[   0.         48.60335     0.       ...  449.15677     0.\n",
      "       0.      ]\n",
      "   [   0.          0.          0.       ...    0.        163.86298\n",
      "     362.05234 ]\n",
      "   [   0.          0.          0.       ...    0.        495.19412\n",
      "     527.6653  ]\n",
      "   ...\n",
      "   [   0.          0.          0.       ...    0.          0.\n",
      "       0.      ]\n",
      "   [   0.          0.          0.       ...    0.          0.\n",
      "       0.      ]\n",
      "   [ 282.34436     0.          0.       ... 2211.1577      0.\n",
      "     626.2237  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 323.05542     0.          0.       ...   48.972565    0.\n",
      "     390.94504 ]\n",
      "   [ 143.44275     0.          0.       ...    0.        169.13965\n",
      "       0.      ]\n",
      "   [   0.          0.          0.       ...   56.235703  151.6111\n",
      "      29.438757]\n",
      "   ...\n",
      "   [   0.          0.       2602.2239   ...  483.5842      0.\n",
      "       0.      ]\n",
      "   [  24.848997    0.       2178.2297   ...  505.60916     0.\n",
      "       0.      ]\n",
      "   [ 188.09912     0.          0.       ...    0.        225.06454\n",
      "     152.85992 ]]\n",
      "\n",
      "  [[  10.483081    0.          0.       ...    0.          0.\n",
      "      71.85386 ]\n",
      "   [  77.31349     0.          0.       ...  327.67926    63.076748\n",
      "     387.79053 ]\n",
      "   [   0.          0.          0.       ...    0.          0.\n",
      "       0.      ]\n",
      "   ...\n",
      "   [   0.          0.       1873.2329   ...  470.06094   464.76086\n",
      "       0.      ]\n",
      "   [ 233.85252     0.          0.       ...  307.31284     0.\n",
      "       0.      ]\n",
      "   [ 740.31885     0.          0.       ...    0.       1186.9946\n",
      "       0.      ]]\n",
      "\n",
      "  [[   0.        292.51804     0.       ...    0.          0.\n",
      "       0.      ]\n",
      "   [   0.        432.6928      0.       ...  135.62193   117.29592\n",
      "       0.      ]\n",
      "   [   0.        506.02985     0.       ...    0.          0.\n",
      "       0.      ]\n",
      "   ...\n",
      "   [ 166.53638  1538.8888      0.       ...    0.          0.\n",
      "       0.      ]\n",
      "   [ 452.97928   262.27713     0.       ...    0.          0.\n",
      "       0.      ]\n",
      "   [ 908.6935    541.052       0.       ...    0.       1451.1732\n",
      "       0.      ]]]]\n"
     ]
    }
   ],
   "source": [
    "# Extract features from an arbitrary intermediate layer with VGG19\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "base_model = VGG19(weights='imagenet')\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)\n",
    "\n",
    "img_path = 'original_gauge.png'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "block4_pool_features = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
