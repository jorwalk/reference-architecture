{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to the localhost client\n",
    "client = MongoClient()\n",
    "\n",
    "# Get the petfinder database\n",
    "db = client.petfinder_database\n",
    "collection = db.pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total size of the collection.\n",
    "collection.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total size of collection without an image\n",
    "collection.count_documents({\"url_img\":{\"$eq\":\"\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store collection into Pandas DataFrame\n",
    "import pandas as pd\n",
    "# Set ipython's max row display\n",
    "pd.set_option('display.max_row', 1000)\n",
    "\n",
    "# Set iPython's max column width to 50\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "query = {}\n",
    "cursor = collection.find(query)\n",
    "df =  pd.DataFrame(list(cursor))\n",
    "# Delete the _id\n",
    "del df['_id']\n",
    "\n",
    "df.head(4)\n",
    "df.size\n",
    "df[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby([\"type\"] ).size().to_frame(name = 'count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df1.plot(kind='bar', y='count', x='type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby([\"type\"] ).size().to_frame(name = 'count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the description\n",
    "df2 = df[\"desc\"]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def substitute(keywords):\n",
    "    rep = {\n",
    "        \"&\": \"and\",\n",
    "        \"(\": \"\",\n",
    "        \")\": \"\",\n",
    "        \"/\":\"\",\n",
    "        \"'\":\"\",\n",
    "        \".\":\"\",\n",
    "        \"`\":\"\",\n",
    "        \",\":\"\"\n",
    "    }\n",
    "    rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    \n",
    "    return pattern.sub(lambda m: rep[re.escape(m.group(0))], keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_type = set()\n",
    "agg_breed = set()\n",
    "\n",
    "def desc_parse(record):\n",
    "    keyword = 'adoptable '\n",
    "    before_keyword, keyword, after_keyword = record.partition(keyword)\n",
    "    # clean and sanitize the string\n",
    "    clean = substitute(after_keyword)\n",
    "    split = clean.split(' ')\n",
    "    \n",
    "    # First Record is Type of Pet\n",
    "    pet_type = split[0]\n",
    "    \n",
    "    # Second Record is the Age\n",
    "    age = split[1]\n",
    "    \n",
    "    # Third record is the Gender\n",
    "    gender = split[2]\n",
    "    \n",
    "    # Records are Breed\n",
    "    breed = \" \".join(split[3:])\n",
    "    \n",
    "    agg_type.add(pet_type)\n",
    "    agg_breed.add(breed)\n",
    "    \n",
    "    return pet_type, age, gender, breed\n",
    "    \n",
    "    \n",
    "df2.apply(desc_parse)\n",
    "# print(\"-- Types --\")\n",
    "# print(agg_type)\n",
    "print(\"-- Breeds --\")\n",
    "print(len(agg_breed))\n",
    "# agg_breed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requestImage(src):\n",
    "    uu = str(uuid.uuid4())\n",
    "    req = urllib.request.Request(\n",
    "        src,\n",
    "        data=None,\n",
    "        headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/542.36'\n",
    "        }\n",
    "    )\n",
    "    with urllib.request.urlopen(req) as response, open('./img-dog/'+uu+'.jpg', 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "\n",
    "    return uu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.count_documents({\"type\":\"CAT\",\"url_img\":{\"$ne\":\"\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file with image URIs and labels\n",
    "import csv\n",
    "f = open('dogs__labels.csv', 'w')\n",
    "with f:\n",
    "    fieldnames = ['pet']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting = csv.QUOTE_NONE, delimiter='|', quotechar='')\n",
    "    writer.writeheader()\n",
    "    \n",
    "#     for doc in collection.find({\"type\":\"DOG\",\"state\":\"MO\", \"url_img\":{\"$ne\":\"\"}}).limit( 1244 ):\n",
    "    for doc in collection.find({\"type\":\"DOG\",\"state\":\"MO\", \"url_img\":{\"$ne\":\"\"}}).limit( 1244 ):\n",
    "        pet_type, age, gender, breed = desc_parse(doc['desc'])\n",
    "\n",
    "        base_str = \"gs://ociautomltest-vcm/{0}.jpg,{1}\"\n",
    "    \n",
    "#         entry = base_str.format(doc['id'], breed.replace(\" \", \"_\"))\n",
    "        entry = base_str.format(doc['id'], \"DOG\")\n",
    "        \n",
    "        writer.writerow({'pet': entry})\n",
    "#         writer.writerow(entry)\n",
    "#     requestImage(doc['url_img'])\n",
    "# CSV Format\n",
    "# gs://ociautomltest-vcm/flowers/images/5217892384_3edce91761_m.jpg,dandelion,tulip,rose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new DataFrame for the agg_breed\n",
    "# df_breed = pd.DataFrame(list(agg_breed),columns=['breed'])\n",
    "# df_breed.size\n",
    "# df_breed.head()\n",
    "# # df_breed.info\n",
    "\n",
    "# #List unique values in the df['name'] column\n",
    "\n",
    "# num_breeds = df_breed.breed.unique()\n",
    "# len(num_breeds)\n",
    "\n",
    "\n",
    "# df_breed.plot(kind='line', x='breed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Scikit Learn\n",
    "Latent Dirichlet Allocation (LDA) is a algorithms used to discover the topics that are present in a corpus. A few open source libraries exist, but if you are using Python then the main contender is Gensim. Gensim is an awesome library and scales really well to large text corpuses. Gensim, however does not include Non-negative Matrix Factorization (NMF), which can also be used to find topics in text. The mathematical basis underpinning NMF is quite different from LDA. I have found it interesting to compare the results of both of the algorithms and have found that NMF sometimes produces more meaningful topics for smaller datasets. NMF has been included in Scikit Learn for quite a while but LDA has only recently (late 2015) been included. The great thing about using Scikit Learn is that it brings API consistency which makes it almost trivial to perform Topic Modeling using both LDA and NMF. Scikit Learn also includes seeding options for NMF which greatly helps with algorithm convergence and offers both online and batch variants of LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = agg_breed\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 3\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "print('-----')\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
